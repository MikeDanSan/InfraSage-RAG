# Observability for AI Systems

## Why Observability Matters Here

Traditional web services have well-understood observability patterns: request latency, error rates, throughput. AI systems have all of those plus a new category of concerns: **quality, cost, and non-determinism.**

An LLM can return a 200 OK with a perfectly formatted response that is completely wrong. Your HTTP metrics won't catch that. This is why AI observability requires thinking beyond standard APM.

**DevOps connection**: You already know the three pillars (logs, metrics, traces). AI systems add a fourth concern: **evaluation** (is the output actually good?). For this project, we focus on the first three plus cost tracking.

## What to Measure

### Per-Request Metrics

| Metric | Where | Why |
|--------|-------|-----|
| Embedding latency | embeddings.py | Time to embed the user question |
| Search latency | retrieval.py | Time for FAISS to return results |
| LLM latency | llm.py | Time for the completion API call (the bottleneck) |
| Total request latency | main.py | End-to-end time for the user |
| Prompt token count | llm.py | Input tokens sent to the LLM |
| Completion token count | llm.py | Output tokens generated by the LLM |
| Total token count | llm.py | Sum of above, drives cost |
| Retrieval scores | retrieval.py | Similarity scores for returned chunks (quality signal) |
| Top-k count | retrieval.py | How many chunks were retrieved |
| Estimated cost | llm.py | Dollar cost of this request |

### Per-Ingestion Metrics

| Metric | Where | Why |
|--------|-------|-----|
| PDF page count | ingestion.py | Document size |
| Chunk count | ingestion.py | How many chunks were created |
| Total embedding tokens | embeddings.py | Tokens used during ingestion |
| Ingestion latency | main.py | Time to process the entire document |

## Latency Breakdown

Understanding where time is spent is critical for production systems:

```
Total query time: ~2-5 seconds (typical)
  |-- Embed question:     100-300ms  (API call to OpenAI)
  |-- FAISS search:       <10ms      (local, in-memory)
  |-- Prompt construction: <1ms      (string formatting)
  |-- LLM completion:     1-4s       (API call to OpenAI, dominates)
```

The LLM completion is 80-95% of your latency. If you need to make the system faster, the levers are:
- Use a smaller/faster model (gpt-4o-mini vs gpt-4o)
- Reduce output length (lower max_tokens)
- Enable streaming (user sees tokens as they arrive)
- Cache frequent queries

## Cost Tracking

AI systems have a direct, per-request cost that traditional software doesn't. Every API call costs money, and it scales linearly with usage.

### How to Calculate Cost

```
cost = (prompt_tokens * input_price_per_token) + (completion_tokens * output_price_per_token)
```

Store pricing as configuration so you can update it when models change:

```python
MODEL_PRICING = {
    "gpt-4o-mini": {"input": 0.15 / 1_000_000, "output": 0.60 / 1_000_000},
    "gpt-4o": {"input": 2.50 / 1_000_000, "output": 10.00 / 1_000_000},
}
```

### What to Log

For every LLM call, log:
- Model name
- Prompt tokens
- Completion tokens
- Calculated cost
- Timestamp

This gives you the data to answer: "How much did we spend on AI this week?" and "What's our average cost per query?"

## Retrieval Quality Signals

Retrieval scores (the distances returned by FAISS) are a quality signal:

- **High similarity scores across all top-k results**: The document likely covers this topic well
- **Low similarity scores**: The question might be outside the document's scope
- **Big gap between #1 and #2 result**: One chunk is clearly the best match
- **All scores similar**: The question is vague or the chunks are too similar

You can use this to:
- Set a minimum similarity threshold (don't use chunks below a certain score)
- Warn the user when retrieval confidence is low
- Log quality metrics for later analysis

## Structured Logging

Use Python's built-in `logging` module with structured output. For each request, log a JSON object with all the metrics. This makes it easy to parse logs with tools you already know (CloudWatch, ELK, Datadog).

Example log entry:
```json
{
  "event": "query_completed",
  "question": "How do I configure ECS?",
  "embedding_latency_ms": 215,
  "search_latency_ms": 3,
  "llm_latency_ms": 2840,
  "total_latency_ms": 3062,
  "prompt_tokens": 1423,
  "completion_tokens": 287,
  "total_tokens": 1710,
  "estimated_cost_usd": 0.000386,
  "top_k": 5,
  "top_score": 0.87,
  "model": "gpt-4o-mini"
}
```

## What You'll Build (Phase 2)

In Phase 2, you'll add:

1. Timing decorators or context managers to measure latency per stage
2. Token counting using tiktoken (before the API call) and the API response (after)
3. Cost calculation based on configurable model pricing
4. Structured logging that outputs JSON-formatted metrics
5. A cost estimation that prints after each request

## AWS Production Perspective

When you eventually deploy this:

- Logs go to CloudWatch Logs (or OpenSearch)
- Metrics go to CloudWatch Metrics or Prometheus
- You'd set alarms on: cost per hour, error rate, p95 latency
- You'd build a dashboard showing: requests/min, avg cost/request, token usage trends
- Budget alerts in AWS Billing for the OpenAI API spend (tracked via your logs)

This is where your DevOps background directly applies.

## Key Takeaways

- AI systems need cost and quality observability, not just uptime metrics
- The LLM call dominates both latency and cost
- Log token usage and calculated cost for every request
- Retrieval scores are a quality signal you can monitor
- Structured JSON logging makes your AI system observable with standard DevOps tools
- Your AWS/DevOps background maps directly to productionizing these concerns
